{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JhWAo1KDW1N"
      },
      "source": [
        "# Práctica 1: construyendo un modelo de clasificación de imágenes\n",
        "\n",
        "En este notebook se muestra cómo crear un modelo de clasificación de imágenes utilizando las técnicas vistas en clase. A lo largo del notebook aparecen una serie de preguntas que debes responder.\n",
        "\n",
        "Para crear nuestro clasificador de imágenes vamos a utilizar la librería [fastAI](https://www.fast.ai/). Este notebook está inspirado en el curso asociado a dicha librería.\n",
        "\n",
        "En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLpxKT4fwuG7"
      },
      "source": [
        "## Librerías\n",
        "\n",
        "Comenzamos descargando la última versión de la librería FastAI. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -> Reiniciar Entorno de ejecución)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uj9Perd_G8yQ"
      },
      "outputs": [],
      "source": [
        "!pip install fastai -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f--H3rbwuG-"
      },
      "source": [
        "A continuación, cargamos aquellas librerías que son necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xn1xZElYCgmf"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOXqTHaDwuG_"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Para esta práctica vamos a usar como ejemplo de dataset el [Intel Image Classification dataset](https://www.kaggle.com/puneet6060/intel-image-classification). Este dataset consta de imágenes de tamaño 150x150 distribuidas en 6 categorías (buildings, forest, glacier, mountain, sea, street). Los siguientes comandos descargan y descomprimen dicho dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jrWISrVEwuHA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://unirioja-my.sharepoint.com/:u:/g/personal/joheras_unirioja_es/EbMVHqKMSnNHh6I0-4-QWdQBlVDKz2Uz5Ky73zc5tHGofg?download=1 -O IntelImageClassification.zip\n",
        "!unzip IntelImageClassification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOZ9_1bfwuHB"
      },
      "source": [
        "Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto [Path](https://docs.python.org/3/library/pathlib.html) que apunta al directorio que acabamos de crear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PlvjTmaSwuHB"
      },
      "outputs": [],
      "source": [
        "path = Path('IntelImageClassification/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAojFRJfwuHB"
      },
      "source": [
        "Con el objeto path podemos utilizar funciones como `ls()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3-dd0D_HwuHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66b14d4-07b5-4416-e472-7e2715b56ccf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('IntelImageClassification/test'),Path('IntelImageClassification/train')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "path.ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PmBu5eGwuHD"
      },
      "source": [
        "Vemos que nuestro dataset consta de dos carpetas llamadas `train` y `test`. Recordar que es importante hacer la partición del dataset en dos conjuntos distintos, para luego poder evaluarlo correctamente. Podemos ahora crear objetos `path` que apunten respectivamente a nuestro conjunto de entrenamiento y a nuestro conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9AD_R2afwuHD"
      },
      "outputs": [],
      "source": [
        "trainPath = path/'train'\n",
        "testPath = path/'test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHhN5L55wuHF"
      },
      "source": [
        "Veamos el contenido de cada uno de estos directorios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HNS_aNmMKPhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4ba1ac-6978-4a21-91c7-575e84fbdc3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#6) [Path('IntelImageClassification/train/forest'),Path('IntelImageClassification/train/glacier'),Path('IntelImageClassification/train/buildings'),Path('IntelImageClassification/train/sea'),Path('IntelImageClassification/train/street'),Path('IntelImageClassification/train/mountain')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "trainPath.ls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VdemXalhwuHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d71094-24a5-47ac-86d9-bb1b7a949037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#6) [Path('IntelImageClassification/test/forest'),Path('IntelImageClassification/test/glacier'),Path('IntelImageClassification/test/buildings'),Path('IntelImageClassification/test/sea'),Path('IntelImageClassification/test/street'),Path('IntelImageClassification/test/mountain')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "testPath.ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldkBrJwjwuHF"
      },
      "source": [
        "Podemos ver que tanto la carpeta `train` como la carpeta `test` contienen 6 subcarpetas, una por cada clase del dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp_a3yGewuHF"
      },
      "source": [
        "**Ejercicio**\n",
        "\n",
        "¿Qué contiene la carpeta `buildings` de la carpeta `test`? Ejecuta a continuación el comando necesario para comprobarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LPuFj3imwuHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2e07b0-6c0c-4f49-d1d9-168db462c388"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#437) [Path('IntelImageClassification/test/buildings/21339.jpg'),Path('IntelImageClassification/test/buildings/21646.jpg'),Path('IntelImageClassification/test/buildings/23833.jpg'),Path('IntelImageClassification/test/buildings/20550.jpg'),Path('IntelImageClassification/test/buildings/23846.jpg'),Path('IntelImageClassification/test/buildings/20766.jpg'),Path('IntelImageClassification/test/buildings/20666.jpg'),Path('IntelImageClassification/test/buildings/22812.jpg'),Path('IntelImageClassification/test/buildings/20465.jpg'),Path('IntelImageClassification/test/buildings/24160.jpg')...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "buildings = testPath/'buildings'\n",
        "buildings.ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " La carpeta buildings de la carpeta test contiene 437 objetos del formato .jpg.\n"
      ],
      "metadata": {
        "id": "TqV_7Gw2cU4Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4GCJATVwuHF"
      },
      "source": [
        "## Cargando el dataset\n",
        "\n",
        "A continuación vamos a mostrar cómo se carga el dataset para poder posteriormente crear nuestro modelo. Este proceso se hace en dos pasos. Primero se construye un objeto `DataBlock` y a continuación se construye un objeto `DataLoader` a partir del `DataBlock`. Tienes más información sobre estos objetos en la documentación de [FastAI](https://docs.fast.ai/tutorial.datablock.html).\n",
        "\n",
        "### Datablock\n",
        "\n",
        "Comenzamos construyendo el objeto `DataBlock`. A continuación explicaremos cada una de sus componentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxYPGDlIwuHG"
      },
      "outputs": [],
      "source": [
        "db = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
        "                 get_items=get_image_files,\n",
        "                 splitter=RandomSplitter(valid_pct=0.2,seed=42),\n",
        "                 get_y=parent_label,\n",
        "                 item_tfms = Resize(256),\n",
        "                 batch_tfms=[*aug_transforms(size=128,min_scale=0.75),Normalize.from_stats(*imagenet_stats)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j2XWhlQwuHG"
      },
      "source": [
        "Vamos a ver las distintas componentes del `DataBlock`.\n",
        "\n",
        "- El atributo `blocks` sirve para indicar el tipo de nuestros datos. Como estamos en un problema de clasificación de imágenes, tenemos que la entrada de nuestro modelo será una imagen, es decir un `ImageBlock`, y la salida será una categoría, es decir un `CategoryBlock`. Por lo tanto indicamos que `blocks = (ImageBlock, CategoryBlock)`.\n",
        "- El atributo `get_items` debe proporcionar una función para leer los datos. En nuestro caso queremos leer una serie de imágenes que estarán almacenadas en un `path`. Para ello usamos la función `get_image_files`. Puedes ver qué hace exactamente esta función ejecutando el comando `??get_image_files`.\n",
        "- El atributo `splitter` nos indica cómo partir el dataset. Daros cuenta que tenemos un conjunto de entrenamiento y uno de test, pero para entrenar nuestro modelo y probar distintas alternativas nos interesa usar un conjunto de validación, que lo vamos a tomar de forma aleatorea a partir de nuestro conjunto de entrenamiento usando un 20% del mismo. Para ello usaremos el objeto `RandomSplitter(valid_pct=0.2,seed=42)`.\n",
        "- El atributo `get_y` sirve para indicar cómo extraemos la clase a partir de nuestros datos. La función `get_image_files` nos proporciona una lista con los paths a las imágenes de nuestro dataset. Si nos fijamos en dichos paths, la clase de cada imagen viene dada por la carpeta en la que está contenida, por lo que podemos usar el método `parent_label` para obtener la clase de la misma.\n",
        "\n",
        "Por último, los atributos `item_tfms` y `batch_tfms` sirven para aplicar una técnica conocida como *preescalado* (o *presizing*).   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okzvWedXwuHH"
      },
      "source": [
        "### Preescalado (presizing)\n",
        "\n",
        "El preescalado es una técnica de aumento de datos diseñada para minimizar la destrucción de datos. Para poder sacar el máximo partido a las GPUs, es necesario que todas las imágenes tengan el mismo tamaño, por lo que es común reescalar todas las imágenes al mismo tamaño.\n",
        "\n",
        "Sin embargo, hay varias técnicas de aumento que si se aplican después de reescalar pueden introducir zonas vacías o degradar los datos. Por ejemplo, si rotamos una imagen 45 grados, los bordes de la imagen quedan vacíos, lo que no le sirve para nada al modelo. Para solucionar este problema se utiliza la técnica del preescalado que consta de dos pasos.\n",
        "1. Las imágenes se reescalan a una dimensión mayor que la que se usará realmente para entrenar.\n",
        "2. Se aplican las distintas técnicas de aumento, y finalmente se reescala al tamaño deseado.\n",
        "\n",
        "El punto clave es el primer paso que sirve para crear imágenes con el suficiente espacio para luego poder aplicar los distintos aumentos. Tienes más información sobre esta técnica en el [libro de FastAI](https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb).\n",
        "\n",
        "En nuestro caso estamos haciendo un escalado inicial a tamaño 256 para luego aplicar un escalado a tamaño 128. Notar que no sólo estamos aplicando un escalado como técnica de aumento de datos, sino que también gracias a la función `aug_transforms` se aplican otros aumentos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srNHlfC8wuHH"
      },
      "source": [
        "### Data augmentation\n",
        "\n",
        "Como hemos visto en clase, la técnica de aumento de datos (o *data augmentation*) nos proporciona un método para aumentar el tamaño de nuestro dataset. FastAI ofrece una serie de aumentos por defecto que se pueden configurar mediante el método `aug_transforms`. Veamos a continuación que aumentos ofrece dicha función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1V3KFKOwuHI"
      },
      "outputs": [],
      "source": [
        "??aug_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwvLWqmawuHI"
      },
      "source": [
        "Como vemos la función anterior puede ser utilizada para fijar distintos aumentos y la probabilidad con la que queremos que se apliquen. En caso de querer otro tipo de transformaciones que no estén incluidas por defecto en dicha función podemos usar la librería [Albumentations](https://github.com/albumentations-team/albumentations) como se explica en la documentación de [FastAI](https://docs.fast.ai/tutorial.albumentations.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z1W5lfbwuHI"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "Pasamos ahora a construir nuestro `DataLoader` que se construye a partir del `DataBlock` construido anteriormente indicándole el path donde se encuentran nuestras imágenes. Además podemos configurar el `DataLoader` indicándole el tamaño del batch que queremos utilizar. Al trabajar con GPUs es importante que usemos batches de tamaño 2^n para optimizar el uso de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_qM-eb_wuHI"
      },
      "outputs": [],
      "source": [
        "dls = db.dataloaders(trainPath,bs=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U53rAlTWwuHJ"
      },
      "source": [
        "A continuación mostramos un batch de nuestro `DataLoader`. Es conveniente comprobar que realmente se han cargado las imágenes y sus anotaciones de manera correcta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0XRb1iDK6Ah"
      },
      "outputs": [],
      "source": [
        "dls.show_batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGOdPngwuHK"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Pasamos ahora a construir y entrenar nuestro modelo. Pero antes vamos a definir una serie de *callbacks*.\n",
        "\n",
        "### Callbacks\n",
        "\n",
        "En ocasiones nos interesa cambiar el comportamiento por defecto que tiene el proceso de entrenamiento, por ejemplo para guardar los mejores pesos que se han producido hasta ese momento. El procedimiento usado por FastAI para incluir dicha funcionalidad son los *callbacks* que sirven para modificar el proceso de entrenamiento. La lista completa de callbacks incluida en FastAI, está disponible en su [documentación](https://docs.fast.ai/callback.core.html). En nuesto caso sólo vamos a utilizar 3 callbacks:\n",
        "- ShowGraphCallback: este callback sirve para mostrar las curvas de entrenamiento y validación.\n",
        "- EarlyStoppingCallback: este callback nos permite aplicar la técnica de early stopping. Para ello debemos indicarle la métrica que queremos monitorizar para saber cuándo parar, y la paciencia (es decir cuántas épocas dejamos que el modelo continúe entrenando si no ha habido mejora).\n",
        "- SaveModelCallback: este callback guarda el mejor modelo encontrado durante el proceso de entrenamiento y lo carga al final del mismo. Como vamos a crear un modelo usando la arquitectura resnet18 conviene que indiquemos esto en el nombre del modelo. También sería conveniente indicar el nombre del dataset para no confundirlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6w9MvuuwuHK"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    ShowGraphCallback(),\n",
        "    EarlyStoppingCallback(patience=3),\n",
        "    SaveModelCallback(fname='modelResnet18')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYSAX-44wuHK"
      },
      "source": [
        "Además de estos tres callbacks utilizaremos otro que nos servirá para acelerar el entrenamiento de nuestros modelos usando [*mixed precision*](https://docs.fast.ai/callback.fp16.html) - este callback se indica de manera diferente como veremos a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh8l_CGpwuHK"
      },
      "source": [
        "## Construyendo el modelo\n",
        "\n",
        "A continuación construimos nuestro modelo, un objeto de la clase `Learner`, utilizando el método `cnn_learner` que toma como parámetros el `DataLoader`, la arquitectura que queremos entrenar (en nuestro caso un `resnet18`), la métrica que usaremos para evaluar nuestro modelo (esta evaluación se hace sobre el conjunto de validación, y en nuestro caso será la *accuracy*), y los callbacks. Notar que en la instrucción anterior incluimos la transformación del modelo a *mixed precision* mediante la instrucción ``to_fp16``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jncYVdouwuHK"
      },
      "outputs": [],
      "source": [
        "learn = vision_learner(dls,resnet18,metrics=accuracy,cbs=callbacks).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvVTr1ouwuHK"
      },
      "source": [
        "Notar que internamente la función `cnn_learner` hace varias cosas con la arquitectura que le pasamos como parámetro (en este caso `resnet18`). Dicha arquitectura fue entrenada inicialmente para el problema de ImageNet, por lo que ante una nueva imagen, su salida sería la predicción en una de las 1000 clases de ImageNet. Sin embargo, internamente la función `cnn_learner` elimina las últimas capas de dicha arquitectura, y las reemplaza con una adecuada para nuestro problema concreto.\n",
        "\n",
        "Antes de entrenar nuestro modelo debemos encontrar un *learning rate* adecuado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIbcP0dcwuHL"
      },
      "source": [
        "### Learning rate finder\n",
        "\n",
        "Como hemos visto en teoría, el trabajo de [Leslie Smith](https://arxiv.org/abs/1506.01186) proporciona un método para encontrar un learning rate adecuado para entrenar nuestro modelo. Dicho learning rate lo puedes encontrar con la función `lr_find()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeISWn3wwuHL"
      },
      "outputs": [],
      "source": [
        "learn.lr_find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHXDNIXFwuHM"
      },
      "source": [
        "La función anterior no solo nos devuelve un gráfico sino que nos sugiere dos valores `lr_min` y `lr_steep`. La recomendación es utilizar el valor de `lr_steep`, para entrenar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPcm0ysGwuHM"
      },
      "source": [
        "### Fine-tuning\n",
        "\n",
        "A continuación vamos a aplicar la técnica de *fine tuning*. En FastAI esto es tan sencillo como llamar al método `fine_tune` del objeto `Learner`. Este método recibe dos parámetros principalmente, el número de épocas (10 en nuestro caso) y el learning rate. El proceso que sigue para entrenar consiste en:\n",
        "1. Congelar todas las capas salvo la última, y entrenar esa parte del modelo durante una época.\n",
        "2. Descongelar la red, y entrenar el modelo por el número de épocas indicado.\n",
        "\n",
        "Al ejecutar la siguiente instrucción aparecerá una tabla donde podrás ver la pérdida para el conjunto de entrenamiento, la pérdida para el conjunto de validación, y la accuracy para el conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMLibjppQcNt"
      },
      "outputs": [],
      "source": [
        "learn.fine_tune(10,base_lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLivjVGvwuHN"
      },
      "source": [
        "Al final del entrenamiento se ha guardado un modelo en la carpeta `models` que contiene el mejor modelo construido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NWkGHwwuHN"
      },
      "outputs": [],
      "source": [
        "Path('models').ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewi96UEewuHN"
      },
      "source": [
        "Para su uso posterior, es conveniente exportar el modelo. Para ello es necesario en primer lugar convertir el modelo a fp32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LJmKWEHwuHN"
      },
      "outputs": [],
      "source": [
        "learn.to_fp32()\n",
        "learn.export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHHB5aACwuHN"
      },
      "source": [
        "Podemos ver que dicho modelo se ha guardado en el mismo directorio donde nos encontramos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxWnBPOnwuHN"
      },
      "outputs": [],
      "source": [
        "Path().ls(file_exts='.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smbtkWT0wuHN"
      },
      "source": [
        "## Evaluando el modelo\n",
        "\n",
        "Una vez tenemos entrenado nuestro modelo nos interesa saber:\n",
        "1. ¿Qué tal funciona en el conjunto de test?\n",
        "2. ¿Qué errores comete?\n",
        "3. ¿Cómo se utiliza para predecir la clase ante nuevas imágenes?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW6VtiaNwuHR"
      },
      "source": [
        "### Evaluando el modelo en el conjunto de test\n",
        "\n",
        "Para poder evaluar nuestro modelo en el conjunto de test debemos crear un nuevo `DataBlock` y un nuevo `DataLoader`. La única diferencia con el `DataBlock` utilizado previamente es que para hacer la partición del dataset usamos un objeto de la clase `GrandparentSplitter` indicando que el conjunto de validación es nuestro conjunto de test. En el caso del `DataLoader`, la diferencia con el definido anteriormente es que cambiamos la ruta al path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVCN_eZKwuHR"
      },
      "outputs": [],
      "source": [
        "dbTest = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
        "                 get_items=get_image_files,\n",
        "                 splitter=GrandparentSplitter(valid_name='test'),\n",
        "                 get_y=parent_label,\n",
        "                 item_tfms = Resize(256),\n",
        "                 batch_tfms=[*aug_transforms(size=128,min_scale=0.75),Normalize.from_stats(*imagenet_stats)])\n",
        "dlsTest = dbTest.dataloaders(path,bs=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Lf15rUwuHS"
      },
      "source": [
        "Para trabajar con este dataloader debemos modificar nuestro objeto `Learner`. En concreto su atributo `dls`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPccuNUawuHS"
      },
      "outputs": [],
      "source": [
        "learn.dls = dlsTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xH9hR3NwuHS"
      },
      "source": [
        "Ahora podemos evaluar nuestro modelo usando el método `validate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xbaoVXFwuHT"
      },
      "outputs": [],
      "source": [
        "learn.validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0VEVAzXaQps"
      },
      "source": [
        "El método `validate` nos devuelve dos valores: el valor de la función de pérdida, y el valor de nuestra métrica (la accuracy en este caso). Por lo que podemos ver que el modelo tiene una accuracy en el conjunto de test de aproximadamente un 82% (esto puede variar dependiendo de la ejecución)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuSQp5JbwuHO"
      },
      "source": [
        "### Interpretación del modelo\n",
        "\n",
        "Hemos visto que nuestro modelo obtiene una accuracy aproximada (puede variar debido a la aleatoriedad del proceso de entrenamiento) de un 92% en el conjunto de validación. Pero nos interesa conocer los errores que se cometen y si estos son razonables. Para ello podemos construir un objeto `ClassificationInterpretation` a partir de nuestro `Learner` y mostrar la matriz de confusión asociada. Recordar que hemos cambiado el DataLoader en el paso anterior, porque es conveniente volver al dataloader usado inicialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebcJupQd6DtQ"
      },
      "outputs": [],
      "source": [
        "learn.dls=dls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbP42pdzXNZ8"
      },
      "outputs": [],
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix(figsize=(12,12),dpi=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyguOvLlwuHP"
      },
      "source": [
        "Si nos fijamos, la mayoría de errores se producen porque el modelo confunde imágenes de la clase street con imágenes de la clase buildings; e imágenes de la clase mountain con la clase glacier. Podemos ver aquellas en las que se produce un mayor error del siguiente modo. Para ello utilizaremos el comando ``interp.plot_top_losses(10,nrows=2)``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNQS29N_lFJQ"
      },
      "outputs": [],
      "source": [
        "interp.plot_top_losses(10,nrows=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9FUA7KwuHP"
      },
      "source": [
        "A partir de la ejecución anterior, podemos ver que hay algunas imágenes que están mal anotadas, y otras en las que es comprensible por qué se está produciendo el error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdPWq4vbwuHQ"
      },
      "source": [
        "### Usando el modelo\n",
        "\n",
        "Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XlaLbqcUfx8"
      },
      "outputs": [],
      "source": [
        "learn_inf = load_learner('export.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "381tLEPtwuHR"
      },
      "source": [
        "Ahora podemos usar dicho modelo para hacer inferencia con una nueva imagen mediante el método `predict`. En nuestro caso vamos a usar una imagen del conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO1rzeUEwuHR"
      },
      "outputs": [],
      "source": [
        "learn_inf.predict('IntelImageClassification/test/buildings/20057.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugxmoSEswuHR"
      },
      "source": [
        "La función anterior devuelve tres valores:\n",
        "- La clase (buildings en este caso).\n",
        "- El índice asociado a dicha clase.\n",
        "- Las probabilidades para cada una de las categorías.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uoUfn2Ce3NC"
      },
      "source": [
        "## Creando una aplicación para nuestro modelo\n",
        "\n",
        "Es fundamental que los modelos sean usables, por lo que es conveniente proporcionar una interfaz secilla que permita usar nuestros modelos. Para ello, vamos a usar dos herramientas: [Gradio](https://gradio.app/) y los [espacios de HuggingFace](https://huggingface.co/spaces). En concreto vamos a ver cómo construir la siguiente [aplicación](https://huggingface.co/spaces/AP2223/Practica1).\n",
        "\n",
        "En primer lugar es necesario que crees una [cuenta en HuggingFace](https://huggingface.co/join). Este paso sólo hay que realizarlo una vez. Además debes  [crear un token de escritura](https://huggingface.co/docs/hub/security-tokens).\n",
        "\n",
        "Una vez realizados esos dos pasos vamos a descargar una librería que permita subir nuestro modelo a HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LETU2DpmqCle"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub[\"fastai\"] -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTMOIsmjriyf"
      },
      "source": [
        "A continuación nos autenticamos en HuggingFace con el token de escritura creado previamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrtKnnsfp-eH"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWNsWTRmrqXn"
      },
      "source": [
        "Finalmente, para subir nuestro modelo debemos indicar nuestro nombre de usuario de HuggingFace y un nombre para nuestro modelo. En la siguiente instrucción cambia joheras por tu nombre de usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7SLxFcjp-tf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import push_to_hub_fastai\n",
        "\n",
        "# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\n",
        "repo_id = \"joheras/intel-image-classification\"\n",
        "\n",
        "push_to_hub_fastai(learner=learn, repo_id=repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SYsEDjUrzvN"
      },
      "source": [
        "Al ejecutar la celda anterior aparecerá una URL donde podrás encontrar tu modelo. Vamos a ver cómo crear una aplicación con el.\n",
        "\n",
        "1. Crea un [espacio en HuggingFace](https://huggingface.co/new-space). Al crear dicho espacio usamos como nombre ``Practica1`` y seleccionamos Gradio como SDK.\n",
        "2. Descarga dos de las imágenes del dataset.\n",
        "3. Una vez creado el espacio vamos a la pestaña ``Files and versions``. En dicha pestaña debemos:\n",
        "  - Subir las dos imágenes que hayas descargado mediante el botón Add file -> Upload file.\n",
        "  - Crear un fichero requirements.txt mediante el botón Add file -> Create a new file. Este [fichero](https://huggingface.co/spaces/AP2223/Practica1/blob/main/requirements.txt) contendrá las librerías que son necesarias instalar para ejecutar nuestra aplicación, en este caso fastai.\n",
        "  - Crear un fichero app.py mediante el botón Add file -> Create a new file. Este [fichero](https://huggingface.co/spaces/AP2223/Practica1/blob/main/app.py) contendrá el código de la aplicación.\n",
        "4. Siguiendo estos pasos tendrás una aplicación que podrás ver desde la pestaña ``App`` (la construcción de la aplicación puede llevar unos segundos, el proceso de construcción lo podrás ver pulsando en el botón ``See logs``.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practica1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}